#!/usr/bin/env python
#

import os
import sys
import random
import hashlib
import urllib2
import socket
import string
import time
import ConfigParser
import json

import pymongo
import cherrypy

minute = 60
hour = 60 * minute
day = 24 * hour
week = 7 * day
month = 30 * day
year = 365 * day

# Ensure that all operations are done in UTC
os.environ['TZ'] = 'UTC'
time.tzset()


class Config(ConfigParser.RawConfigParser):
	def getlist(self, section, option):
		return [x.strip() for x in self.get(section, option).split(',')]


def cacheduration(t):
	# Cache duration is a stepwise function of request period
	durations = [
		# (if req period >, set cache duration to)
		(0, hour),       # always cache at least 1 hour
		(2*week, day),   # cache 2-week views for 1 day
		(6*month, week), # cache 6-month views for 1 week
	]

	duration = 0
	for treq, tdur in durations:
		if t > treq:
			duration = tdur
	return duration


class timer(object):
	def __init__(self):
		self._start = None

	def start(self):
		self._start = time.time()
		return self

	def check(self):
		return time.time() - self._start

	def stop(self):
		self._stop = time.time()
		return self._stop - self._start


class frob(dict):
	def __getattr__(self, key):
		return self[key]
	def __setattr__(self, key, value):
		self[key] = value


class crowapp(object):
	exposed = True

	def __init__(self, mw, handler):
		self.mw = mw
		self.handler = handler
		_ = hashlib.md5()
		_.update(str(time.time()) + str(id(self)))
		self.instance = handler.__name__ + '.' + _.hexdigest()[:8]
		self.reqno = 0
		self.dbserver, _ = self.mw.config.getlist('db', 'default')
		self.client = pymongo.MongoClient(self.dbserver)

	@cherrypy.tools.accept(media='application/json')
	@cherrypy.tools.json_out()
	@cherrypy.tools.json_in()
	def POST(self, *args, **kwargs):
		self.reqno += 1
		handler = self.handler(self, cherrypy.request, self.reqno)
		return handler(*args, **kwargs)

	def getDB(self, pool):
		if not pool.startswith('crow_'):
			pool = 'crow_' + pool
		if hasattr(self.client, pool):
			return getattr(self.client, pool)
		return None


class crowhandler(object):
	cacheignores = []

	try:
		import parsedatetime
		pdt = parsedatetime.Calendar()
	except ImportError:
		pdt = None

	def __init__(self, app, req, reqno):
		self.app = app
		self.request = req
		self.reqno = reqno
		self.reqid = self.app.instance + '.' + str(self.reqno)
		self._usecache = True

	def log(self, msg):
		print '[%s %.3f] %s' % (self.reqid, time.time(), msg)
		sys.stdout.flush()

	def tconv(self, s):
		'''Converts time string to Epoch seconds (int).
		If parsedatetime available, we can convert various English
		expressions of time.  If not, we can only convert Epoch
		seconds as a string to Epoch seconds as int.
		'''

		try:
			return int(s)
		except ValueError:
			tm, mode = self.pdt.parse(s)
			ptm = list(tm)
			tmnow = list(time.gmtime())

			def _(tm):
				if mode == 1:
					tm = tm[:3] + [0, 0, 0] + tm[6:]
					# if future, subtract a year
					if time.mktime(tm) > time.mktime(tmnow):
						tm[0] -= 1
						return _(tm)
				if mode == 2:
					tm = tmnow[:3] + tm[3:]
					# if future, subtract a day
					if time.mktime(tm) > time.mktime(tmnow):
						return time.mktime(tm) - 86400
				return time.mktime(tm)

			return _(ptm)

	def querykey(self, query):
		md5 = hashlib.md5()
		md5.update(self.__class__.__name__)	# method/path, e.g. "starts"
		for key in sorted(query.keys()):
			if key in self.cacheignores:
				continue
			md5.update(key)
			md5.update(str(query[key]))
		return md5.hexdigest()[:8]

	def usecache(self, req, params):
		if params['current']:
			return False
		if 'X-Crow-RCache' in req.headers and req.headers['X-Crow-RCache'] == 'no':
			return False
		return True

	def cached(self, req, db, query):
		qkey = self.querykey(query)
		if 'X-Crow-RCache' in req.headers and req.headers['X-Crow-RCache'] == 'no':
			return qkey, None, 'nocache'
		r = db.cache.find_one({'_id': qkey})
		if r is None:
			return qkey, None, 'uncached'
		if r['expires'] < time.time():
			# expired
			return qkey, None, 'expired (%.3fs)' % (r['expires'] - time.time())
		return qkey, r['data'], 'ok'

	def cache(self, db, query, data, lifetime=10*minute):
		qkey = self.querykey(query)
		r = db.cache.find_one({'_id': qkey})
		if r:
			db.cache.update({'_id': qkey},
			                {'expires': time.time() + lifetime,
			                 'data': data})
		else:
			db.cache.insert({'_id': qkey,
			                 'expires': time.time() + lifetime,
			                 'data': data})
		return qkey

	def logquery(self, db, query):
		qkey = self.querykey(query)
		r = db.qstat.find_one({'_id': qkey})
		if r:
			db.qstat.update({'_id': qkey},
			                {'count': r['count'] + 1,
			                 'last': time.time(),
			                 'query': json.dumps(query)})
		else:
			db.qstat.insert({'_id': qkey,
			                 'count': 1,
			                 'last': time.time(),
			                 'query': json.dumps(query)})
		return qkey

	def mongofilter(self, f, expr):
		if ',' in expr:
			users = expr.split(',')
			return {'$or': [f(user) for user in users]}
		else:
			return f(expr)

	@staticmethod
	def _or(*args):
		return { '$or': args }

	@staticmethod
	def _and(*args):
		return { '$and': args }

	def standardfilter(self, params, startTime, interval):
		andFilters = []

		# jobs must have been queued before our interval ended
		began_before_interval = {'lifecycle.queued': {'$lt': startTime + interval}}

		# jobs must either be currently in queue...
		in_queue = {'lifecycle.unobserved': 0}

		# ...or have completed after our interval began
		completed_after_interval = {'lifecycle.unobserved': {'$gt': startTime}}

		andFilters.append(began_before_interval)
		andFilters.append(self._or(in_queue, completed_after_interval))

		if (params.project == 'all'):
			andFilters.append({'latest.ProjectName': {'$exists': True}})
		else:
			andFilters.append(self.mongofilter(lambda p: {'latest.ProjectName': p}, params.project))

		if (params.user != 'all'):
			andFilters.append(self.mongofilter(lambda u: {'latest.User': {'$regex': u + '@*'}}, params.user))

		if (params.task != 'all'):
			andFilters.append({'latest.ClusterId': int(params.task)})

		select = self._and(*andFilters)
		return select

	def readparams(self, params):
		params = frob(params)
		req = self.request.json
		for key in params.keys():
			if key in req:
				params[key] = type(params[key])(req[key])
		return params

	def tdomain(self, params):
		ct = int(time.time())
		if params.start == 'default' and params.end == 'default':
			interval = params.hours * 3600
			startTime = ct - interval
		elif params.start == 'default':
			interval = params.hours * 3600
			startTime = self.tconv(params.end) - interval
		else:
			interval = params.hours * 3600
			startTime = self.tconv(params.start)
		return ct, startTime, interval


class current(crowhandler):
	exposed = True

	cacheignores = [
		# n.b. width doesn't affect queried data, so ignore that in /current queries
		'width',
		# current doesn't either - it only changes whether we check for cached data
		'current',
	]

	def __call__(self, *args, **kwargs):
		params = self.readparams({
			'pool': 'default',
			'project': 'all',
			'user': 'all',
			'task': 'all',
			'groupby': 'ProjectName',
			'current': False,		# euphemism: current == True -> no caching please
		})

		db = self.app.getDB(params.pool)

		t = timer().start()
		rows = db.jobs.find(
			self._and(
				{'latest.QDate': {'$gt': time.time() - (14*day)}},
				{'latest.JobStatus': {'$in': [0, 1, 2, 5, 7]}},
			),
			{
				'latest.QDate': 1,
				'latest.JobStatus': 1,
				'latest.ProjectName': 1,
				'latest.JobStartDate': 1,
				'latest.CompletionDate': 1,
				'latest.User': 1,
				'latest.ClusterId': 1,
				'latest.Owner': 1,
			})
		self.log('t(mongodb) = %.5fs' % t.check())

		count = 0
		for r in rows:
			count += 1
		self.log('%d matching jobs' % count)

		result = {
			'plot': [
				{
					'name': 'member1',
					'data': [
						[time1, data1],
						[time2, data2],
					]
				},
				{
					'name': 'member2',
					'data': [
						[time1, data1],
						[time2, data2],
					]
				},
			]
		}

		return result


class starts(crowhandler):
	exposed = True

	# QDate = 1410041462
	# JobCurrentStartDate = 1410041468
	# JobStartDate = 1410041468
	# LastMatchTime = 1410041468
	# JobCurrentStartExecutingDate = 1410041470
	# CompletionDate = 1410041471
	# EnteredCurrentStatus = 1410041471
	# JobFinishedHookDone = 1410041471
	# LastJobLeaseRenewal = 1410041471

	cacheignores = [
		# n.b. width doesn't affect queried data, so ignore that in /current queries
		'width',
		# current doesn't either - it only changes whether we check for cached data
		'current',
	]

	def __call__(self, *args, **kwargs):
		self.log('initial postdata: %s' % self.request.json)

		params = self.readparams({
			'pool': 'default',
			'project': 'all',
			'user': 'all',
			'task': 'all',
			'groupby': 'ProjectName',
			'rel': 'running',
			'start': 'default',
			'end': 'default',
			'hours': 0,
			'bins': 0,
			'current': False,		# euphemism: current == True -> no caching please
		})

		db = self.app.getDB(params.pool)

		ct, startTime, interval = self.tdomain(params)
		self.log('interval = %d + %d (%s++)' % (startTime, interval, time.ctime(startTime)))

		if not params.bins:
			params.bins = params.hours
		binwidth = interval / params.bins

		if startTime % 3600:
			# if not an exact hour boundary, do one extra bin
			params.bins += 1
			startTime -= (startTime % 3600)

		usecache = self.usecache(self.request, params)
		if usecache:
			self.logquery(db, params)

		# cache check
		if usecache:
			t = timer().start()
			qkey, data, reason = self.cached(self.request, db, params)
			if data:
				r = json.loads(data)
				self.log('t(cache check %s %s) = %.5fs' % (qkey, reason, t.check()))
				return r
			else:
				self.log('t(cache check %s %s) = %.5fs' % (qkey, reason, t.check()))
		else:
			self.log('no-cache')

		# need to tune the db selection based on $rel
		select = self.standardfilter(params, startTime, interval)

		t = timer().start()
		rows = db.jobs.find(
			select,
			{
				'latest.QDate': 1,
				'latest.JobStatus': 1,
				'latest.ProjectName': 1,
				'latest.JobStartDate': 1,
				'latest.CompletionDate': 1,
				'latest.User': 1,
				'latest.ClusterId': 1,
				'latest.Owner': 1,
			})
		self.log('t(mongodb) = %.5fs' % t.check())
		self.log('selector found %d jobs' % rows.count())

		result = {}
		result['plot'] = []

		_running = lambda stime, etime: \
		                  (int((stime - startTime) / binwidth),
			               int((etime - startTime) / binwidth))

		_started = lambda stime, etime: \
		                  (int((stime - startTime) / binwidth),
			               int((stime - startTime) / binwidth))

		_finished = lambda stime, etime: \
		                   (int((etime - startTime) / binwidth),
			                int((etime - startTime) / binwidth))

		rel = {
			'default': (_running, 'JobStartDate'),
			'running': (_running, 'JobStartDate'),
			'started': (_started, 'JobStartDate'),
			'finished': (_finished, 'JobStartDate'),
			'submitted': (_started, 'QDate'),
		}

		# aggregate, slice, chop, dice, puree

		t = timer().start()
		series = rows.distinct('latest.' + params.groupby) 
		pData = {}
		totals = {}
		for p in series:
			pData[p] = []
			totals[p] = 0
			for b in range(params.bins):
				binstart = startTime + (b * binwidth)
				pData[p].append([binstart * 1000, 0])
		self.log('t(distinct) %.5fs' % t.check())

		t = timer().start()
		keyprop = rel[params.rel][1]
		for r in rows:
			group = r['latest'][params.groupby]
			if keyprop in r['latest']:
				stime = r['latest'][keyprop]
			else:
				stime = ct+1
			if 'CompletionDate' in r['latest']:
				etime = r['latest']['CompletionDate']
			else:
				etime = ct
			first, last = rel[params.rel][0](stime, etime)
			if first < 0:
				first = 0
			if last > params.bins:
				last = params.bins
			#if first > last:
			#	print 'anomalous', r['_id']
			for b in range(first, last+1):
				try:
					pData[group][b][1] += 1
					totals[group] += 1
				except IndexError:
					pass
		self.log('t(binning) = %.5fs' % t.check())

		# not sure why yet but some of these groups come up with zero counts.
		# (this happens when the select finds jobs that binning excludes.)
		for group in list(pData.keys()):
			#total = sum([pData[group][x][1] for x in range(len(pData[group]))])
			#if total == 0:
			#	del pData[group]
			if totals[group] == 0:
				del pData[group]

		for p in series:
			if p in pData:
				ser = {}
				ser['name'] = p + ' (' + str(totals[p]) + ')'
				ser['data'] = pData[p]
				result['plot'].append(ser)

		t = timer().start()
		# even if we asked for no caches, we will take advantage and cache this result
		qkey = self.cache(db, params, json.dumps(result), lifetime=cacheduration(interval))
		self.log('t(cache %s) = %.5fs' % (qkey, t.check()))
		return result


class distincts(crowhandler):
	'''not used'''
	exposed = True
	@cherrypy.tools.accept(media='application/json')
	@cherrypy.tools.json_out()
	@cherrypy.tools.json_in()

	def __call__(self, *args, **kwargs):
		req = self.request.json

		db = self.app.getDB(req['pool'])

		interval = 720 * 3600
		if 'interval' in req:
			interval = req['interval'] * 3600

		ret = {}
		ret['Tasks'] = []
		ret['ProjectNames'] = []
		startTime = int(time.time())-int(interval)
		rows = db.jobs.find({'latest.CompletionDate': {'$gt': startTime}},{'latest.ProjectName': 1,'latest.User': 1,'latest.ClusterId': 1,'latest.Owner': 1})
		ret['ProjectNames'] = rows.distinct('latest.ProjectName')
		ret['Tasks'] = rows.distinct('latest.ClusterId')
		ret['Users'] = rows.distinct('latest.User')
		ret['Owners'] = rows.distinct('latest.Owner')
		for r in range(len(ret['Users'])):
			ret['Users'][r] = ret['Users'][r].split('@')[0]
		# print ret
		return ret


class crowmw(object):
	'''Crow middleware'''
	exposed = True

	def __init__(self, config):
		self.config = config
		self.starts = crowapp(self, starts)
		self.current = crowapp(self, current)


if __name__ == '__main__':	  
	# cherrypy.tools.CORS = cherrypy.Tool('before_finalize', CORS)

	cfg = Config()

	# Read .ini file from relative to $0
	base = os.path.basename(sys.argv[0])
	home = os.path.dirname(sys.argv[0])
	parent = os.path.dirname(home)
	if os.path.split(home)[-1] == 'bin':
		home = parent
	if os.path.exists(os.path.join(parent, 'etc')):
		home = parent

	file = 'crow.ini'
	for path in ('/etc', file), (home, 'etc', file), (home, file):
		file = os.path.join(*path)
		#print 'Trying %s ...' % file
		cfg.read(file)

	file = base + '.ini'
	for path in ('/etc', file), (home, 'etc', file), (home, file):
		file = os.path.join(*path)
		#print 'Trying %s ...' % file
		cfg.read(file)

	cherrypy.config.update({'tools.log_headers.on': False})
	cherrypy.quickstart(crowmw(cfg), '/', '/web/crow/server/crow-server.conf')

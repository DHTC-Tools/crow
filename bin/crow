#!/usr/bin/env python

import os
import sys
import time
import getopt
import ConfigParser
import signal

import pymongo
import htcondor
import classad

try:
	from setproctitle import setproctitle, getproctitle
except ImportError:
	setproctitle = getproctitle = lambda t: True

try:
	from hashlib import md5
except ImportError:
	# py2.4
	from md5 import md5

DefaultConfig = '''
[crow]
debug = false
brand = generic
detach = false
log = /var/log/crow-%(brand)s-%(threadid)s.log
pidfile = /var/run/crow-%(brand)s-%(threadid)s.pid

[db]
# This section associates database logical names (DLNs) to (server, database)
# pairs.  The DLNs are referenced below, in [queue] and [history] sections.
schedd = mongodb://mongodbserver:27017, crow_%(brand)s
collector = mongodb://mongodbserver:27017, crow_coll_%(brand)s

# Watcher configurations: entries in [queue] and [history] sections
# are in this format:
#   source = database[, database][, interval=30]
# "source" varies for each section.  Database is a DLN defined above in [dn],
# and interval, if specified, is how many seconds should elapse between
# probes to a data source.

[queue]
# Source is "local" for the local schedd, or a hostname for a remote schedd.
# (The latter is untested, though.)
local = schedd, interval=60

[history]
# Source is a file name containing condor history records.
/var/lib/condor/spool/history = schedd, interval=30

[classads]
# This adfilter will accept all classads. Use a Matcher expression
# to exclude or include classads by name.
adfilter = /.*/

# matcher-expression := [!]<matcher-atom> [<matcher-expression> logical-conjunction]
# matcher-atom := <regular-expression> | <glob-expression> | <literal-expression>
# regular-expression := /[regular expression tokens]/
# glob-expression := [fnmatch glob, containing ? or *]
# literal-expression := [anything else]
# logical-conjunction := OR | AND | NOT
'''

JobStates = (
	'unexpanded',
	'idle',
	'running',
	'removed',
	'completed',
	'held',
	'submission-error',
	'suspended',
)



class Config(ConfigParser.RawConfigParser):
	def getlist(self, section, option):
		return [x.strip() for x in self.get(section, option).split(',')]


def tfmt(t):
	return '%.3f (%s)' % (float(t), time.ctime(t))


class CrowError(Exception):
	pass


class compoundfunction(list):
	'''I like this but we're not using it atm.'''
	def __call__(self, *args, **kwargs):
		for f in self:
			if f(*args, **kwargs) is False:
				return False
		return True


class frob(dict):
	def __getattr__(self, key):
		return self[key]
	def __setattr__(self, key, value):
		self[key] = value


class DriverStack(object):
	class InappropriateDriverError(CrowError):
		pass

	@classmethod
	def _try(cls, *args):
		raise cls.InappropriateDriverError, '%s is a metadriver' % cls.__name__

	@classmethod
	def open(cls, *args):
		symbols = globals()
		for sym, value in symbols.items():
			if type(value) != type(cls):
				continue
			if not issubclass(value, cls):
				continue
			try:
				return value._try(*args)
			except cls.InappropriateDriverError, e:
				pass
		raise cls.InappropriateDriverError, 'no suitable drivers for "%s"' % args[0]


class Logger(DriverStack):
	singleton = None

	def __init__(self, spec):
		# we can offer multiple log types
		pass

	def __call__(self, fmt, *args):
		pass

	@classmethod
	def NullLogger(cls):
		if cls.singleton is None:
			cls.singleton = cls(None)
		return cls.singleton


class FileLogger(Logger):
	@classmethod
	def _try(cls, *args):
		if args[0].startswith('file:'):
			return cls(*args)
		if args[0].startswith('/'):
			return cls(*args)
		raise cls.InappropriateDriverError, '%s is not a file spec' % args[0]

	def __init__(self, spec):
		# we could offer other log types
		self.fp = open(spec, 'w')

	def __call__(self, fmt, *args):
		if args:
			print >>self.fp, fmt % args
		else:
			print >>self.fp, fmt
		self.fp.flush()

	def __del__(self):
		self.close()

	def close(self):
		self.fp.close()


class FileDescLogger(FileLogger):
	@classmethod
	def _try(cls, *args):
		if args[0].startswith('fd:'):
			return cls(*args)
		if args[0].lower() in ('stdout', 'stderr', '-'):
			return cls(*args)
		raise cls.InappropriateDriverError, '%s is not a filedesc spec' % args[0]

	def __init__(self, spec):
		if spec.startswith('fd:'):
			fd = int(spec[3:])
			if fd == 1:
				self.fp = sys.stdout
				self.close = lambda: None
			elif fd == 2:
				self.fp = sys.stderr
				self.close = lambda: None
			else:
				self.fp = os.fdopen(fd, 'w')
		elif spec.lower() == 'stdout' or spec == '-':
			self.fp = sys.stdout
			self.close = lambda: None
		elif spec.lower() == 'stderr':
			self.fp = sys.stderr
			self.close = lambda: None

class Watcher(object):
	def __init__(self, hooks={}, interval=30, logger=None):
		self.hooks = {
			'begin': lambda w, jobs: True,
			'cycle': lambda w, jobs: True,
			'end': lambda w, jobs: True,
		}
		self.hooks.update(hooks)
		self.interval = interval

		if logger is None:
			logger = Logger.NullLogger()
		self.log = logger

	def setlog(self, logger):
		self.log = logger

	@classmethod
	def parse(cls, spec):
		dbs = []
		params = {}
		for arg in spec.split(','):
			if '=' in arg:
				k, v = arg.split('=', 1)
				params[k.strip()] = v.strip()
			else:
				dbs.append(arg.strip())
		return dbs, params


class QueueWatcher(Watcher):
	'''QueueWatcher creates a relationship to a schedd or collector, so
	that it can be queried for running jobs.'''

	def __init__(self, schedd, collector=None, hooks={}, interval=30, logger=None):
		Watcher.__init__(self, hooks=hooks, interval=interval, logger=logger)

		if schedd == 'local':
			schedd = None

		if schedd:
			if not collector:
				raise TypeError, 'QueueWatcher with nonlocal schedd requires collector name also'
			def reconnect():
				coll = htcondor.Collector(collector)
				ad = coll.locate(htcondor.DaemonTypes.Schedd, schedd)
				self.source = htcondor.Schedd(ad)

		elif collector:
			def reconnect():
				self.source = htcondor.Collector(collector)

		else:
			def reconnect():
				self.source = htcondor.Schedd()

		self.reconnect = reconnect
		self.reconnect()


	@staticmethod
	def stringify(snapshot):
		'''Ensure that each value in a job snapshot (a dict of
		classad names and values) is a base type:
		int, float, bool, or str.'''
		new = {}
		for k, v in snapshot.items():
			if type(v) in (long, float, bool, str):
				new[k] = v
			else:
				new[k] = str(v)
		return new


	def watch(self, jobactions):
		self.hookctx = frob()
		while True:
			self.hookctx.count = 0

			try:
				jobs = self.source.query()
			except Exception, e:
				self.log('crow queue %s: %s (reconnecting)', time.strftime('%Y%m%dT%H%M%S'), str(e))
				self.reconnect()
				continue

			jobactions.begin(jobs)

			if self.hooks['begin'](self, jobs) is False:
				break
			for job in jobs:
				try:
					job = self.stringify(job)
					self.hookctx.count += 1
					job = jobactions(job)
				except Exception, e:
					self.log(str(e))

			jobactions.end(jobs)

			if self.interval is None:
				break

			if self.hooks['end'](self, jobs) is False:
				break
			time.sleep(self.interval)


class HistoryWatcher(Watcher):
	def __init__(self, file, hooks={}, interval=30, logger=None):
		Watcher.__init__(self, hooks=hooks, interval=interval, logger=logger)
		self._file = file
		self._current = {}
		self._fp = None


	@staticmethod
	def parsevalue(value):
		'''Convert a classad value to an unquoted string, boolean,
		int, or float as appropriate.'''
		if value.startswith('"'):
			return value.strip('"')

		if value == 'true':
			return True

		if value == 'false':
			return False

		if '.' in value:
			try:
				return float(value)
			except ValueError:
				pass

		try:
			return int(value)
		except ValueError:
			pass

		return value


	def read(self):
		# This lets us catch updates to the current file
		self._fp.seek(self._fp.tell())

		recs = []
		for line in self._fp:
			if line.startswith('***'):
				# end of record
				yield self._current
				self._current = {}
				continue

			ad, value = line.strip().split(' = ', 1)
			self._current[ad] = self.parsevalue(value)


	def watch(self, jobactions):
		self.hookctx = frob()

		# Wait for history file to become available
		while not self._fp:
			try:
				s = os.stat(self._file)
				inode = s.st_ino
				self._fp = open(self._file, 'r')
			except OSError:
				self._fp = None
				if self.interval is None:
					return
				time.sleep(self.interval)

		# history file now exists, and is open

		s = None
		while True:
			self.hookctx.count = 0

			# Make sure history file still exists
			try:
				s = os.stat(self._file)
			except OSError:
				# no file (yet?)
				s = None
			if not s and self.interval is not None:
				if self._fp:
					self._fp.close()
					self._fp = None
				if not s:
					time.sleep(self.interval)
					continue

			jobactions.begin([])

			# Ready to read current file
			if self._fp:
				if self.hooks['begin'](self, None) is False:
					break
				for job in self.read():
					self.hookctx.count += 1
					job = jobactions(job)

			jobactions.end([])

			if self._fp and self.hooks['end'](self, None) is False:
				break

			if self.interval is None:
				self._fp.close()
				break

			sys.stdout.flush()

			# Check whether history file has cycled
			if s and s.st_ino != inode:
				# Old file moved, and new history file available
				if self.hooks['cycle'](self, None) is False:
					break
				inode = s.st_ino
				self._fp.close()
				self._fp = open(self._file, 'r')
				# no interval delay if file cycles - scan new one immediately

			else:
				time.sleep(self.interval)



class CrowDB(DriverStack):
	'''Provides a base algorithm for updating a database with job
	information, abstracted over get/set methods that are implemented
	by a subclass.  See CrowMongo below for example.'''

	class DriverFailed(CrowError): pass

	def __init__(self, *args, **kwargs):
		if 'logger' in kwargs:
			self.log = kwargs['logger']
		else:
			self.log = Logger.NullLogger()

	def setlog(self, logger):
		self.log = logger

	@staticmethod
	def jobhash(rec):
		'''Compute a unique identifier for an instantaneous job state
		by hashing each classad value (ordered by sorted classad names).
		This will be used to determine whether we already know about this
		exact job state or "version".'''
		m = md5()
		for k in sorted(filter(lambda k: not k.startswith('_'), rec.keys())):
			m.update(str(rec[k]))
		return m.hexdigest()[:16]

	def setopts(self, *args, **kwargs):
		pass

	# Given a job description (snapshot), ensure that the job is up to
	# date in the database by updating or inserting records as needed.
	def update(self, job):
		jobid = job['GlobalJobId']
		hash = self.jobhash(job)

		jobent = self.jobent(jobid)
		if jobent:
			# Already present.  Check for significant changes.
			# Latest is a full job snapshot, not a reference.
			latest, lhash = self.latest(jobid, jobent)

			if hash == lhash:
				# No changes whatsoever.
				self.log('+ no change to %s', jobid)

			# This is odd: condor history gives two 'EnteredCurrentStatus' values.
			# I don't think it's a problem so long as the later one is second in
			# the history file.  (It will overwrite the prior when creating dicts.)
			# If this turns out problematic, we can add logic to choose the better
			# value; in most cases it will be the larger number.

			elif latest['EnteredCurrentStatus'] == job['EnteredCurrentStatus']:
				# No change of state.  Simply update latest.
				self.log('+ updating %s to latest', jobid)
				self.setlatest(jobid, job, hash)

			else:
				# State change occurred.  Copy latest to history, then save
				# current as latest.
				self.addhistory(jobid, jobent, latest, lhash)
				self.setlatest(jobid, job, hash)

				# Log changes.
				statelog = JobStates[latest['JobStatus']]
				if latest['JobStatus'] != job['LastJobStatus']:
					statelog += ' -> ' + JobStates[job['LastJobStatus']]
				statelog += ' -> ' + JobStates[job['JobStatus']]
				self.log('+ augmenting history on %s (state %s at %s)',
				         jobid, statelog, tfmt(job['EnteredCurrentStatus']))

		else:
			# No jobent -> never seen before.  Enter ONLY a jobent, no history.
			self.log('+ adding job %s', jobid)
			self.addjob(jobid, job, hash)

	def inqueue(self):
		'''Return all jobs that are believed to be in queue.'''
		return []

	def unsee(self, job):
		'''Mark a job as no longer in queue.'''
		return False

	def derivetimes(self, job):
		'''Derives from a job's timestamps and status the key lifecycle
		event times.  (Dequeue time is the tricky one.)'''
		t = frob({
			'queued': 0,      # entered job queue
			'started': 0,     # first started on worker
			'completed': 0,   # completed on worker (if completed; removed jobs not complete)
			'dequeued': 0,    # removed from queue
			'unobserved': 0,  # first time job known not to be in queue
			# if unobserved > 0, job is historical. if == 0, it's believed to be in queue
		})
		keys = job.keys()
		if 'QDate' in keys:
			t.queued = job['QDate']
		if 'JobStartDate' in keys:
			t.started = job['JobStartDate']
		if 'CompletionDate' in keys:
			t.completed = job['CompletionDate']
		if t.completed > 0:
			t.dequeued = t.completed
		elif t.completed == 0:
			s = 0
			if 'JobStatus' in job:
				s = job['JobStatus']
			if s in (0, 1, 2, 5):
				# still in queue
				pass
			elif s in (3, 4, 6, 7):
				# left the building
				if 'EnteredCurrentStatus' in keys:
					t.dequeued = job['EnteredCurrentStatus']
		return t


class CrowMongo(CrowDB):
	'''Implements a MongoDB store for job history that follows the
	common update logic in CrowDB above.'''

	@classmethod
	def _try(cls, *args):
		if args[0].startswith('mongodb:'):
			return cls(*args)
		raise cls.InappropriateDriverError, '%s is not a mongodb database spec' % args[0]

	def __init__(self, *args, **kwargs):
		CrowDB.__init__(self, *args, **kwargs)

		addr = args[0]
		if len(args) > 1:
			dbname = args[1]
		else:
			dbname = 'crow'

		try:
			conn = pymongo.MongoClient(addr)
		except AttributeError:
			conn = pymongo.Connection(addr)
		except pymongo.errors.ConnectionFailure, e:
			raise self.DriverFailed, str(e)
		self.db = conn[dbname]

	# A jobent record has three keys:
	#   _id: the GlobalJobId from condor classads
	#   snapshots: a list of valid snapshot ids
	#   latest: the most recent snapshot stored
	#           (** not necessarily the most recent observed **)
	#   latestid: the snapshot id (hash) of latest

	def jobent(self, jobid):
		return self.db.jobs.find_one({"_id": jobid})

	def latest(self, jobid, jobent):
		return jobent['latest'], jobent['latestid']

	def jobhistory(self, jobid, jobent):
		return jobent['history']

	def inqueue(self):
		'''Return all jobs that are believed to be in queue.'''
		rows = self.db.jobs.find({'lifecycle.unobserved': 0})
		jobs = []
		for row in rows:
			jobs.append(row['latest'])
		return jobs

	def unsee(self, job):
		'''Mark a job as no longer in queue.
		If job already has lifecycle.unobserved, leave it.  But if zero,
		update with -either- completion time or current time.
		'''
		jobid = job['GlobalJobId']
		jobent = self.db.jobs.find_one({'_id': jobid}, {
			'lifecycle.completed': 1,
			'lifecycle.dequeued': 1,
			'lifecycle.unobserved': 1,
		})
		if jobent is None:
			return False
		if 'lifecycle' in jobent and 'unobserved' in jobent['lifecycle'] and jobent['lifecycle']['unobserved']:
			return False
		try:
			updatewith = jobent['lifecycle']['dequeued'] or jobent['lifecycle']['completed'] or int(time.time())
		except KeyError:
			updatewith = int(time.time())
		self.db.jobs.update({'_id': jobid}, {'$set': {'lifecycle.unobserved': updatewith}})
		return True

	def setlatest(self, jobid, job, hash):
		self.db.jobs.update({'_id': jobid}, {'$set': {
			'latest': job,
			'latestid': hash,
			'lifecycle': self.derivetimes(job),
		}})

	def addhistory(self, jobid, jobent, job, hash):
		hist = self.jobhistory(jobid, jobent) + [hash]

		try:
			job['_id'] = hash
			self.db.snapshots.insert(job)
		except pymongo.errors.DuplicateKeyError:
			pass
		self.db.jobs.update({'_id': jobid}, {'$set': {'history': hist}})

	def addjob(self, jobid, job, hash):
		jobent = {
			'_id': jobid,
			'history': [],
			'lifecycle': self.derivetimes(job),
			'latest': job,
			'latestid': hash,
		}
		self.db.jobs.insert(jobent)


class CrowDebug(CrowDB):
	@classmethod
	def _try(cls, *args):
		if args[0] == 'debug':
			return cls(*args)
		raise cls.InappropriateDriverError, '%s is not a debug database spec' % args[0]

	def __init__(self, *args, **kwargs):
		CrowDB.__init__(self, *args, **kwargs)
		self.pprint = __import__('pprint')
		self.extradebug = False

	def setopts(self, *args, **kwargs):
		if 'level' in kwargs:
			self.level = kwargs['level']

	def update(self, job):
		self.log('%s (%d classads)', job['GlobalJobId'], len(job.keys()))
		if self.level > 1:
			self.log(self.pprint.pformat(job))


##### Lifted from tanglib
import re
import fnmatch

class Matcher(list):
	OR = 1
	AND = 2
	NOT = 3

	@staticmethod
	def matchfunc(expr):
		negate = False
		negstr = ['!', '']

		if expr.startswith('!'):
			negate = True
			expr = expr[1:].strip()

		if expr.startswith('/') and expr.endswith('/'):
			expr = expr[1:-1]
			rx = re.compile(expr, re.I)
			f = lambda x: negate ^ bool(rx.search(x))
			f.desc = negstr[negate] + 're(' + expr + ')'

		elif '*' in expr or '?' in expr or '[' in expr:
			f = lambda x: negate ^ bool(fnmatch.fnmatch(x, expr))
			f.desc = negstr[negate] + 'fnmatch(' + expr + ')'

		else:
			f = lambda x: negate ^ (x.lower() == expr.lower())
			f.desc = negstr[negate] + 'str(' + expr + ')'

		return f


	def _selector(self, expr):
		expr = expr.strip()
		if expr.lower() == 'or':
			yield self.OR
			return
		if expr.lower() == 'and':
			yield self.AND
			return
		if expr.lower() == 'not':
			yield self.NOT
			return

		yield self.matchfunc(expr)
		return


	def __init__(self, exprs, **kwargs):
		selectors = []
		for expr in exprs:
			for sel in self._selector(expr):
				selectors.append(sel)
		list.__init__(self, selectors)

	def describe(self):
		desc = ''
		for item in self:
			if item == self.OR:
				desc += ' or'
			elif item == self.AND:
				desc += ' and'
			elif item == self.NOT:
				desc += ' not'
			else:
				desc += ' ' + item.desc
		return desc.strip()

	def _run(self, key):
		match = []
		for selector in self:
			if selector == self.AND:
				r = match[-2] and match[-1]
				match = match[:-2] + [r]
			elif selector == self.OR:
				r = match[-2] or match[-1]
				match = match[:-2] + [r]
			elif selector == self.NOT:
				match[-1] = not match[-1]
			else:
				match.append(selector(key))
		return match[-1]

	def __call__(self, args, key=lambda x: x):
		stack = []
		return [arg for arg in args if self._run(key(arg))]

	def __contains__(self, other):
		return self._run(other)
##### End of snitch from tanglib


class preparsed(object):
	def __init__(self, value):
		self.value = value


class JobAction(list):
	'''A JobAction is a sequence of JobActions.
	Each action takes a job (a set of ClassAds) as input, and returns
	the same job.  It may alter the job in transit; such actions are
	job filters.
	'''
	def __call__(self, job):
		for action in self:
			job = action(job) or job
		return job

	def begin(self, jobs):
		'''Prep work before starting a round of job updates.'''
		for action in self:
			action.begin(jobs)

	def end(self, jobs):
		'''Final work after ending a round of job updates.'''
		for action in self:
			action.end(jobs)


class DBUpdater(JobAction):
	'''A job action that updates a database with the job.'''
	def __init__(self, name, db, fromhistory=False):
		self.name = name
		self.db = db
		self.fromhistory = fromhistory	# is this job record from history?

	def __call__(self, job):
		# XXX This can be better optimized wrt lifecycle mgt.
		# First we update, which sets lifecycle.*, then we set
		# lifecycle.unobserved differently. That's two updates
		# when one would do.
		self.db.update(job)
		if self.fromhistory:
			self.db.unsee(job)
		return job

	def begin(self, jobs):
		if jobs:
			# find all jobs that are allegedly in queue (unobserved == 0)
			alleged = self.db.inqueue()

			# find jobs that are actually in queue and index them
			actual = {}
			for job in jobs:
				actual[job['GlobalJobId']] = job

			# remove from alleged set all the jobs that were actually found
			wrong = filter(lambda job: job['GlobalJobId'] not in actual, alleged)

			# 'unsee' all the wrong allegations
			for job in wrong:
				jobid = job['GlobalJobId']
				print 'unseeing', jobid
				self.db.unsee(job)

class NullAction(JobAction):
	'''A job action that does nothing to the job; a placeholder.'''
	def __call__(self, job):
		return job


class AdFilter(JobAction):
	'''A job action that selects only classads matching a Matcher
	expression.'''

	def __init__(self, rule):
		self.matcher = Matcher(rule.split(' '))

	def __call__(self, job):
		new = {}
		for key in self.matcher(job.keys()):
			new[key] = job[key]
		return new


class EnsureFilter(JobAction):
	'''A job action that ensures certain classads are defined.'''

	def __init__(self, ensures):
		self.ensures = ensures

	@classmethod
	def fromconfig(cls, config):
		'''Create an EnsureFilter from a ConfigParser instance.'''
		if not config.has_section('ensure'):
			return NullFilter()
		self = cls({})
		for classad, value in config.items('ensure'):
			self.ensures[classad] = preparsed(self.asbest(value))
		return self

	@staticmethod
	def asquoted(value):
		'''Try to parse as a quoted string.'''
		if value.startswith('"') and value.endswith('"'):
			return value[1:-1]
		if value.startswith("'") and value.endswith("'"):
			return value[1:-1]
		return None

	@staticmethod
	def asint(value):
		'''Try to parse as integer.'''
		try:
			return int(value)
		except:
			return None

	@staticmethod
	def asfloat(value):
		'''Try to parse as floating-point.'''
		try:
			return float(value)
		except:
			return None

	@staticmethod
	def asbool(value):
		'''Try to parse as a boolean word.'''
		value = str(value).lower()
		if value in ('0', 'off', 'no', 'false'):
			return False
		elif value in ('1', 'on', 'yes', 'true'):
			return True
		else:
			return None

	@staticmethod
	def asstr(value):
		'''Parse as a string; does not fail.'''
		return str(value)

	def asbest(self, value):
		'''Pick the best way to parse a value.'''
		return self.asquoted(value) or \
		       self.asint(value) or \
		       self.asfloat(value) or \
		       self.asbool(value) or \
		       self.asstr(value)

	def __call__(self, job):
		'''Apply an ensures set.'''
		keys = [k.lower() for k in job.keys()]
		for key, value in self.ensures.items():
			if key.lower() not in keys:
				# value can be of several types:
				if hasattr(value, '__call__'):
					job[key] = value(job)
				elif value.__class__ == preparsed:
					job[key] = value.value
				else:
					job[key] = self.asbest(value)
		return job


class main(object):
	def usage(self, fp=sys.stderr):
		p = os.path.basename(sys.argv[0])
		s = ' ' * len(p)
		print >>fp, 'usage: %s [-D|--debug] [-C|--configfile file]' % p
		print >>fp, '       %s [-d|--database database-uri] [-b|--brand brand]' % s
		print >>fp, '       %s [--detach] command [args...]' % s
		print >>fp, '       %s [opts] [-i|--interval interval] queue' % p
		print >>fp, '       %s [opts] [-i|--interval interval] history [historyfile ...]' % p
		return 2


	def __getitem__(self, k):
		return getattr(self, k)


	def thread(self, subtitle):
		title = self.proctitle + (' [%s]' % subtitle)
		pid = os.fork()
		if pid == 0:
			setproctitle(title)
			return 0
		return pid


	def __call__(self, args):
		self.proctitle = getproctitle()
		self.cfg = Config()

		# Create config using defaults above
		import StringIO
		fp = StringIO.StringIO(DefaultConfig)
		self.cfg.readfp(fp)
		fp.close()

		# Read .ini file from relative to $0
		base = os.path.basename(sys.argv[0])
		home = os.path.dirname(sys.argv[0])
		if os.path.split(home)[-1] == 'bin':
			home = os.path.join(*os.path.split(home)[:-1])

		file = base + '.ini'
		for path in ('/etc', file), (home, 'etc', file), (home, file):
			file = os.path.join(*path)
			#print 'Trying %s ...' % file
			self.cfg.read(file)

		try:
			args = self.getopt(args)
		except getopt.GetoptError, e:
			print >>sys.stderr, str(e)
			print >>sys.stderr
			return self.usage()

		if not args:
			return self.usage()
		cmd = args.pop(0)

		def begin(w, jobs):
			w.hookctx.count = 0
			w.hookctx.start = time.time()
			w.log('** starting scan (%s)', tfmt(time.time()))
		def cycle(w, jobs):
			w.log('** cycled (%s)', tfmt(time.time()))
		def end(w, jobs):
			dt = time.time() - w.hookctx.start
			w.log('-- %d jobs processed in %.2fs (%s)', w.hookctx.count, dt, tfmt(time.time()))

		self.hooks = {
			'begin': begin,
			'cycle': cycle,
			'end': end,
		}

		errs = 0
		self.dbdefs = {}
		for name, dbargs in self.cfg.items('db'):
			# this here replaces %(brand)s in the values in [db] from local vars
			self.dbdefs[name] = [arg.strip() % self for arg in dbargs.split(',')]
		for name in self.globaldbs:
			if name not in self.dbdefs:
				print >>sys.stderr, 'no database named "%s" in configuration' % name
				errs += 1
		if errs:
			return 10

		# This should come after the database existence check above
		if self.debug:
			self.globaldbs.insert(0, 'debug')

		jobactions = JobAction()
		if self.cfg.has_section('ensure'):
			jobactions.append(EnsureFilter.fromconfig(self.cfg))

		if self.cfg.has_option('classads', 'adfilter'):
			jobactions.append(AdFilter(self.cfg.get('classads', 'adfilter') + ' GlobalJobId or'))

		self.threadid = cmd

		if cmd == 'printconf':
			self.cfg.write(sys.stdout)

		elif cmd == 'queue':
			self.queue_all(args, jobactions, parallel=False)

		elif cmd == 'history':
			self.history_all(args, jobactions, parallel=False)

		elif cmd == 'master':
			self.pidfile = self.cfg.get('crow', 'pidfile') % self
			pids = []

			if self.detach:
				pid = self.thread('master')
				if pid:
					return 0
			self.mkpid()

			pids += self.queue_all(args, jobactions, parallel=True)
			pids += self.history_all(args, jobactions, parallel=True)

			try:
				for pid in pids:
					os.waitpid(pid, 0)
			except KeyboardInterrupt:
				for pid in pids:
					os.kill(pid, signal.SIGINT)
			self.rmpid()
			return 0

		else:
			print >>sys.stderr, 'unknown command:', cmd
			return self.usage()


	def getopt(self, args):
		opts, args = getopt.getopt(args, 'hDC:d:b:i:l:',
		                           ['help', 'debug', 'config=', 'db=', 'database=', 'databases=', 'brand=', 'interval=', 'log='])

		for opt, arg in opts:
			if opt in ('-C', '--config'):
				self.cfg.read(arg)

		self.globaldbs = []
		self.brand = self.cfg.get('crow', 'brand')
		self.debug = int(self.cfg.getboolean('crow', 'debug'))
		self.detach = self.cfg.getboolean('crow', 'detach')
		self.interval = None
		self.logfile = None

		for opt, arg in opts:
			if opt in ('-D', '--debug'):
				self.debug += 1
			if opt in ('-d', '--database', '--databases', '--db'):
				self.globaldbs.append(arg)
			if opt in ('--detach',):
				self.detach = True
			if opt in ('-b', '--brand'):
				self.brand = arg
			if opt in ('-i', '--interval'):
				self.interval = int(arg)
			if opt in ('-l', '--log'):
				self.logfile = arg
			if opt in ('-h', '--help'):
				return []

		return args

	def mkpid(self):
		self.pidfile = self.cfg.get('crow', 'pidfile') % self
		self.rmpid()
		os.symlink(str(os.getpid()), self.pidfile)

	def rmpid(self):
		try:
			os.unlink(self.pidfile)
		except:
			pass

	# XXX TODO: collapse queue and history into one method

	def queue(self, source, spec, actions):
		#print 'queue', source, QueueWatcher.parse(spec)
		#return 0
		try:
			self.threadid = 'queue:' + source
			self.mkpid()
			if self.logfile is None:
				self.logfile = self.cfg.get('crow', 'log') % self
			logger = Logger.open(self.logfile)

			dbs, params = QueueWatcher.parse(spec)
			if self.interval:
				params['interval'] = self.interval
			params['hooks'] = self.hooks

			databases = self.globaldbs + dbs
			for name in databases:
				db = CrowDB.open(*self.dbdefs[name])
				db.setlog(logger)
				actions.append(DBUpdater(name, db))

			w = QueueWatcher(source, **params)
			w.setlog(logger)
			w.watch(actions)
			return 0
		except KeyboardInterrupt:
			self.rmpid()
			return 255

	def history(self, source, spec, actions):
		#print 'queue', source, HistoryWatcher.parse(spec)
		#return 0
		try:
			self.threadid = 'history:' + source.replace('/', '_')
			self.mkpid()
			if self.logfile is None:
				self.logfile = self.cfg.get('crow', 'log') % self
			logger = Logger.open(self.logfile)

			dbs, params = HistoryWatcher.parse(spec)
			if self.interval:
				params['interval'] = self.interval
			params['hooks'] = self.hooks

			databases = self.globaldbs + dbs
			for name in databases:
				db = CrowDB.open(*self.dbdefs[name])
				db.setlog(logger)
				actions.append(DBUpdater(name, db, fromhistory=True))

			w = HistoryWatcher(source, **params)
			w.setlog(logger)
			w.watch(actions)
			return 0
		except KeyboardInterrupt:
			self.rmpid()
			return 255

	def parseargument(self, arg):
		'''Parses a commandline argument as a spec, in the fashion of the
		configuration file.  No spaces permitted: spaces separate sources.'''
		if '=' in arg:
			return arg.split('=', 1)
		else:
			return arg, ''

	# XXX TODO: collapse queue_all and history_all into one method

	def queue_all(self, args, actions, parallel=False):
		pids = []

		if args:
			items = [self.parseargument(arg) for arg in args]
		else:
			items = self.cfg.items('queue')

		# ideally we allow specifying single/multiple items on cmd line here
		if parallel:
			for source, spec in items:
				pid = self.thread('queue')
				if pid == 0:
					sys.exit(self.queue(source, spec, actions))
				pids.append(pid)
		else:
			for source, spec in items:
				self.queue(source, spec, actions)
		return pids

	def history_all(self, args, actions, parallel=False):
		pids = []

		if args:
			items = [self.parseargument(arg) for arg in args]
		else:
			items = self.cfg.items('history')

		if parallel:
			for source, spec in items:
				pid = self.thread('history')
				if pid == 0:
					sys.exit(self.history(source, spec, actions))
				pids.append(pid)
		else:
			for source, spec in items:
				self.history(source, spec, actions)
		return pids

if __name__ == '__main__':
	sys.exit(main()(sys.argv[1:]))

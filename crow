#!/usr/bin/env python

import os
import sys
import time
import getopt

import pymongo
import htcondor
import classad

try:
	from hashlib import md5
except ImportError:
	# py2.4
	from md5 import md5

# Needs to be in a config file
DefaultDatabase = 'mongodb://db.mwt2.org:27017'
DefaultHistoryFile = '/var/lib/condor/spool/history'

JobStates = (
	'unexpanded',
	'idle',
	'running',
	'removed',
	'completed',
	'held',
	'submission-error',
)


def tfmt(t):
	return '%.3f (%s)' % (float(t), time.ctime(t))


class QueueWatcher(object):
	'''QueueWatcher creates a relationship to a schedd or collector, so
	that it can be queried for running jobs.'''

	def __init__(self, schedd=None, collector=None, hooks={}, interval=30):
		self.hooks = {
			'begin': lambda w, jobs: True,
			'end': lambda w: True,
		}
		self.hooks.update(hooks)
		self._interval = interval

		if schedd:
			if not collector:
				raise TypeError, 'QueueWatcher with schedd requires collector name also'
			coll = htcondor.Collector(collector)
			ad = coll.locate(htcondor.DaemonTypes.Schedd, schedd)
			self.source = htcondor.Schedd(ad)

		elif collector:
			self.source = htcondor.Collector(collector)

		else:
			self.source = htcondor.Schedd()


	@staticmethod
	def stringify(snapshot):
		'''Ensure that each value in a job snapshot (a dict of
		classad names and values) is a base type:
		int, float, bool, or str.'''
		new = {}
		for k, v in snapshot.items():
			if type(v) in (long, float, bool, str):
				new[k] = v
			else:
				new[k] = str(v)
		return new


	def __iter__(self):
		while True:
			try:
				jobs = self.source.query()
			except Exception, e:
				print >>sys.stderr, str(e)
			if self.hooks['begin'](self, jobs) is False:
				break
			for job in jobs:
				try:
					job = self.stringify(job)
					yield job
				except Exception, e:
					print >>sys.stderr, str(e)

			if self._interval is None:
				break
			if self.hooks['end'](self) is False:
				break
			time.sleep(self._interval)


class HistoryWatcher(object):
	def __init__(self, file, hooks={}, interval=30):
		self.hooks = {
			'begin': lambda w: True,
			'cycle': lambda w: True,
			'end': lambda w: True,
		}
		self.hooks.update(hooks)
		self._interval = interval
		self._file = file
		self._current = {}
		self._fp = None


	@staticmethod
	def parsevalue(value):
		'''Convert a classad value to a string (without quotes), boolean,
		int, or float as appropriate.'''
		if value.startswith('"'):
			return value.strip('"')

		if value == 'true':
			return True

		if value == 'false':
			return False

		if '.' in value:
			try:
				return float(value)
			except ValueError:
				pass

		try:
			return int(value)
		except ValueError:
			pass

		return value


	def read(self):
		# This lets us catch updates to the current file
		self._fp.seek(self._fp.tell())

		recs = []
		for line in self._fp:
			if line.startswith('***'):
				# end of record
				yield self._current
				self._current = {}
				continue

			ad, value = line.strip().split(' = ', 1)
			self._current[ad] = self.parsevalue(value)


	def __iter__(self):
		s = os.stat(self._file)
		inode = s.st_ino
		self._fp = None

		# Wait for history file to become available
		while not self._fp:
			try:
				self._fp = open(self._file, 'r')
				break
			except OSError:
				if self._interval is None:
					return
			time.sleep(self._interval)

		# history file now exists, and is open

		s = None
		while True:
			# Check whether history file has cycled (but only if we are looping)
			if self._interval is not None:
				try:
					s = os.stat(self._file)
				except OSError:
					# old file moved, but no new file yet
					s = None
					self._fp.close()
					self._fp = None
					time.sleep(self._interval)
					continue

			# File exists
			if s and s.st_ino != inode:
				# Old file moved, and new history file available
				if self.hooks['cycle'](self) is False:
					break
				inode = s.st_ino
				self._fp.close()
				self._fp = open(self._file, 'r')

			# Ready to read current file
			if self._fp:
				if self.hooks['begin'](self) is False:
					break
				for snapshot in self.read():
					yield snapshot

			if self._fp and self.hooks['end'](self) is False:
				break

			if self._interval is None:
				self._fp.close()
				break

			sys.stdout.flush()
			time.sleep(self._interval)



class CrowDB(object):
	'''Provides a base algorithm for updating a database with job
	information, abstracted over get/set methods that are implemented
	by a subclass.  See CrowMongo below for example.'''

	@staticmethod
	def jobhash(rec):
		'''Compute a unique identifier for an instantaneous job state
		by hashing each classad value (ordered by sorted classad names).
		This will be used to determine whether we already know about this
		exact job state or "version".'''
		m = md5()
		for k in sorted(filter(lambda k: not k.startswith('_'), rec.keys())):
			m.update(str(rec[k]))
		return m.hexdigest()[:16]

	# Given a job description (snapshot), ensure that the job is up to
	# date in the database by updating or inserting records as needed.
	def update(self, job):
		jobid = job['GlobalJobId']
		hash = self.jobhash(job)

		jobent = self.jobent(jobid)
		if jobent:
			# Already present.  Check for significant changes.
			# Latest is a full job snapshot, not a reference.
			latest, lhash = self.latest(jobid, jobent)

			if hash == lhash:
				# No changes whatsoever.
				print '+ no change to %s' % jobid

			# This is odd: condor history gives two 'EnteredCurrentStatus' values.
			# I don't think it's a problem so long as the later one is second in
			# the history file.  (It will overwrite the prior when creating dicts.)
			# If this turns out problematic, we can add logic to choose the better
			# value; in most cases it will be the larger number.

			elif latest['EnteredCurrentStatus'] == job['EnteredCurrentStatus']:
				# No change of state.  Simply update latest.
				print '+ updating %s to latest' % jobid
				self.setlatest(jobid, job, hash)

			else:
				# State change occurred.  Copy latest to history, then save
				# current as latest.
				self.addhistory(jobid, jobent, latest, lhash)
				self.setlatest(jobid, job, hash)

				# Log changes.
				statelog = JobStates[latest['JobStatus']]
				if latest['JobStatus'] != job['LastJobStatus']:
					statelog += ' -> ' + JobStates[job['LastJobStatus']]
				statelog += ' -> ' + JobStates[job['JobStatus']]
				print '+ augmenting history on %s (state %s at %s)' % \
				      (jobid, statelog, tfmt(job['EnteredCurrentStatus']))

		else:
			# No jobent -> never seen before.  Enter ONLY a jobent, no history.
			print '+ adding job %s' % jobid
			self.addjob(jobid, job, hash)


class CrowMongo(CrowDB):
	'''Implements a MongoDB store for job history that follows the
	common update logic in CrowDB above.'''

	def __init__(self, addr, name, prefix='crow_'):
		try:
			conn = pymongo.MongoClient(addr)
		except AttributeError:
			conn = pymongo.Connection(addr)
		self.db = conn[prefix + name]

	# A jobent record has three keys:
	#   _id: the GlobalJobId from condor classads
	#   snapshots: a list of valid snapshot ids
	#   latest: the most recent snapshot stored
	#           (** not necessarily the most recent observed **)
	#   latestid: the snapshot id (hash) of latest

	def jobent(self, jobid):
		return self.db.jobs.find_one({"_id": jobid})

	def latest(self, jobid, jobent):
		return jobent['latest'], jobent['latestid']

	def jobhistory(self, jobid, jobent):
		return jobent['history']

	def setlatest(self, jobid, job, hash):
		self.db.jobs.update({'_id': jobid}, {'$set': {'latest': job}})
		self.db.jobs.update({'_id': jobid}, {'$set': {'latestid': hash}})

	def addhistory(self, jobid, jobent, job, hash):
		hist = self.jobhistory(jobid, jobent) + [hash]

		try:
			job['_id'] = hash
			self.db.snapshots.insert(job)
		except pymongo.errors.DuplicateKeyError:
			pass
		self.db.jobs.update({'_id': jobid}, {'$set': {'history': hist}})

	def addjob(self, jobid, job, hash):
		jobent = {
			'_id': jobid,
			'history': [],
			'latest': job,
			'latestid': hash,
		}
		self.db.jobs.insert(jobent)


def usage(fp=sys.stderr):
	p = os.path.basename(sys.argv[0])
	print >>fp, 'usage: %s [-d|--database database-uri] [-b|--brand brand] command [args...]' % p
	print >>fp, '       %s [opts] [-i|--interval interval] queue' % p
	print >>fp, '       %s [opts] [-i|--interval interval] history [historyfile ...]' % p
	return 2


def main(args):
	class context(object): pass

	try:
		opts, args = getopt.getopt(args, 'hd:b:i:', ['help', 'database=', 'brand=', 'interval='])
	except getopt.GetoptError, e:
		print >>sys.stderr, str(e)
		print >>sys.stderr
		return usage()

	dbname = DefaultDatabase
	brand = 'generic'
	interval = None

	for opt, arg in opts:
		if opt in ('-d', '--database'):
			dbname = arg
		if opt in ('-b', '--brand'):
			brand = arg
		if opt in ('-i', '--interval'):
			interval = int(arg)
		if opt in ('-h', '--help'):
			return usage()

	if not args:
		return usage()
	cmd = args.pop(0)

	ctx = context()
	def begin(*args):
		ctx.count = 0
		print '** starting scan (%s)' % (tfmt(time.time()))
		sys.stdout.flush()
	def cycle(*args):
		print '** cycled (%s)' % (tfmt(time.time()))
	def end(*args):
		print '-- %d jobs processed (%s)' % (ctx.count, tfmt(time.time()))
		sys.stdout.flush()

	hooks = {
		'begin': begin,
		'cycle': cycle,
		'end': end,
	}

	db = CrowMongo(dbname, brand)

	if cmd == 'queue':
		if interval is None:
			interval = 300
		w = QueueWatcher(interval=interval, hooks=hooks)
		for snapshot in w:
			ctx.count += 1
			db.update(snapshot)

	elif cmd == 'history':
		if interval is None:
			interval = 10
		if args:
			for arg in args:
				# Given args, do a one-time pass over each named file
				w = HistoryWatcher(arg, interval=None, hooks=hooks)
				for snapshot in w:
					ctx.count += 1
					db.update(snapshot)
		else:
			w = HistoryWatcher(DefaultHistoryFile, interval=interval, hooks=hooks)
			for snapshot in w:
				ctx.count += 1
				db.update(snapshot)


if __name__ == '__main__':
	sys.exit(main(sys.argv[1:]))
